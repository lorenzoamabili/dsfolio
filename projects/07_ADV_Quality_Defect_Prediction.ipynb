{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c051c936",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Advanced Quality Analytics\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa4605",
   "metadata": {},
   "source": [
    "# ✅ Advanced Quality Analytics & Predictive Defect Prevention\n",
    "## Portfolio Project 7 — Causal Inference, Survival Analysis, XGBoost-Style Boosting, Shapley Root-Cause, and Statistical Process Capability Forecasting\n",
    "\n",
    "---\n",
    "\n",
    "### What This Notebook Covers (Beyond Basics)\n",
    "| Topic | Technique |\n",
    "|---|---|\n",
    "| Causal inference | Propensity-score matching + treatment-effect estimation |\n",
    "| Survival analysis | Kaplan-Meier curves for time-to-defect |\n",
    "| Advanced boosting | Hand-rolled histogram gradient boosting (XGBoost-style) |\n",
    "| Shapley root-cause | Additive feature attribution for defect probability |\n",
    "| Capability forecasting | Predicting future Cpk drift using time-series regression |\n",
    "| Cost-benefit analysis | Expected cost of defect vs inspection — optimal policy |\n",
    "\n",
    "### Dataset  \n",
    "**Simulated injection-moulding quality log** (10 process vars, 5 quality dims, binary defect)  \n",
    "Structure mirrors Kaggle Steel Defect / UCI Concrete datasets  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c706da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1. Imports ─────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (roc_curve, auc, classification_report,\n",
    "                             confusion_matrix, precision_recall_curve)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print('✓ All imports loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2. Synthetic injection-moulding quality data ─────────\n",
    "def gen_quality_data(n=8000, seed=2025):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ─── Process inputs (10 variables) ───\n",
    "    mold_temp = rng.normal(220, 8, n)\n",
    "    inject_pres = rng.normal(150, 10, n)\n",
    "    hold_time = rng.normal(3.0, 0.3, n)\n",
    "    cool_time = rng.normal(12.0, 1.5, n)\n",
    "    screw_rpm = rng.normal(200, 15, n)\n",
    "    material_mfr = rng.normal(180, 5, n)   # material melt flow rate\n",
    "    humidity = rng.uniform(20, 70, n)\n",
    "    operator_id = rng.integers(0, 8, n)    # 8 operators\n",
    "    shift = rng.choice([0, 1, 2], n)   # 0=day, 1=eve, 2=night\n",
    "    machine_age = rng.uniform(0, 10, n)    # years\n",
    "\n",
    "    # ─── Quality measurements (5 dimensions) ───\n",
    "    wall_thick = 2.0 + 0.003*mold_temp - 0.002 * \\\n",
    "        inject_pres + rng.normal(0, 0.04, n)\n",
    "    surface_fin = 1.5 - 0.005*screw_rpm + 0.01 * \\\n",
    "        material_mfr + rng.normal(0, 0.08, n)\n",
    "    tensile = 45 + 0.2*material_mfr - 0.1*humidity + rng.normal(0, 2.0, n)\n",
    "    shrinkage = 0.8 + 0.002*mold_temp - 0.001 * \\\n",
    "        cool_time + rng.normal(0, 0.05, n)\n",
    "    flash_score = 0.1 + 0.003*inject_pres - \\\n",
    "        0.005*hold_time + rng.normal(0, 0.03, n)\n",
    "\n",
    "    # ─── Defect probability (logistic) ───\n",
    "    logit = (\n",
    "        -4.0\n",
    "        + 0.04 * (mold_temp - 220)\n",
    "        + 0.03 * (inject_pres - 150)\n",
    "        - 0.08 * hold_time\n",
    "        - 0.06 * cool_time\n",
    "        + 0.02 * humidity\n",
    "        + 0.15 * machine_age\n",
    "        + 0.3 * (shift == 2).astype(float)   # night shift effect\n",
    "        + 0.25 * np.abs(wall_thick - 2.0) * 20\n",
    "        + 0.2 * flash_score * 10\n",
    "    )\n",
    "    p_defect = 1 / (1 + np.exp(-logit))\n",
    "    defect = (rng.uniform(0, 1, n) < p_defect).astype(int)\n",
    "\n",
    "    # Time to first defect (for survival analysis) — in minutes since start\n",
    "    # Each sample is ~2 min apart; defect time = cumulative sum of inter-arrival\n",
    "    inter_arrival = rng.exponential(40, n)\n",
    "    time_to_event = np.cumsum(inter_arrival)   # monotonically increasing\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Mold_Temp': mold_temp.round(1), 'Inject_Pres': inject_pres.round(1),\n",
    "        'Hold_Time': hold_time.round(2), 'Cool_Time': cool_time.round(2),\n",
    "        'Screw_RPM': screw_rpm.round(1), 'Material_MFR': material_mfr.round(1),\n",
    "        'Humidity': humidity.round(1), 'Operator_ID': operator_id,\n",
    "        'Shift': shift, 'Machine_Age': machine_age.round(2),\n",
    "        'Wall_Thickness': wall_thick.round(4), 'Surface_Finish': surface_fin.round(4),\n",
    "        'Tensile_Strength': tensile.round(2), 'Shrinkage': shrinkage.round(4),\n",
    "        'Flash_Score': flash_score.round(4),\n",
    "        'Defect': defect,\n",
    "        'Time_min': time_to_event.round(1)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "df = gen_quality_data()\n",
    "PROCESS_VARS = ['Mold_Temp', 'Inject_Pres', 'Hold_Time', 'Cool_Time', 'Screw_RPM',\n",
    "                'Material_MFR', 'Humidity', 'Operator_ID', 'Shift', 'Machine_Age']\n",
    "QUALITY_VARS = ['Wall_Thickness', 'Surface_Finish',\n",
    "                'Tensile_Strength', 'Shrinkage', 'Flash_Score']\n",
    "ALL_FEATS = PROCESS_VARS + QUALITY_VARS\n",
    "\n",
    "print(f'Shape: {df.shape}  |  Defect rate: {df[\"Defect\"].mean()*100:.1f}%')\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c905cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Survival Analysis — Kaplan-Meier Time-to-Defect\n",
    "\n",
    "# Kaplan-Meier estimates the **survival function** S(t) = P(no defect up to time t) without assuming any parametric distribution. We stratify by key risk factors.\n",
    "\n",
    "# ─── 3. Kaplan-Meier estimator (manual) ──────────────────\n",
    "def kaplan_meier(times, events):\n",
    "    \"\"\"\n",
    "    Compute KM survival curve.\n",
    "    times:  array of event/censoring times\n",
    "    events: 1 = event (defect), 0 = censored\n",
    "    Returns (t_km, S_km) — sorted unique event times and survival probs.\n",
    "    \"\"\"\n",
    "    # Sort by time\n",
    "    order = np.argsort(times)\n",
    "    t_sort = times[order]\n",
    "    e_sort = events[order]\n",
    "\n",
    "    unique_times = np.unique(t_sort[e_sort == 1])  # event times only\n",
    "    n_at_risk = len(times)\n",
    "    S = 1.0\n",
    "    t_km, S_km = [0], [1.0]\n",
    "\n",
    "    ptr = 0   # pointer into sorted arrays\n",
    "    for t in unique_times:\n",
    "        # Count events and at-risk at time t\n",
    "        while ptr < len(t_sort) and t_sort[ptr] < t:\n",
    "            n_at_risk -= 1\n",
    "            ptr += 1\n",
    "        # Events at exactly t\n",
    "        d = 0\n",
    "        while ptr < len(t_sort) and t_sort[ptr] == t:\n",
    "            d += e_sort[ptr]\n",
    "            n_at_risk -= 1\n",
    "            ptr += 1\n",
    "        n_at_risk += d   # they were at risk at time t\n",
    "\n",
    "        if n_at_risk > 0:\n",
    "            S *= (1 - d / n_at_risk)\n",
    "        t_km.append(t)\n",
    "        S_km.append(S)\n",
    "        n_at_risk -= d\n",
    "\n",
    "    return np.array(t_km), np.array(S_km)\n",
    "\n",
    "\n",
    "# For survival analysis: treat each defect as an event.\n",
    "# Use cumulative time within each \"batch\" (reset every 200 samples for realism)\n",
    "BATCH = 200\n",
    "survival_rows = []\n",
    "for batch_start in range(0, len(df), BATCH):\n",
    "    batch = df.iloc[batch_start:batch_start+BATCH]\n",
    "    cum_time = 0\n",
    "    for _, row in batch.iterrows():\n",
    "        cum_time += np.random.exponential(2.0)  # ~2 min between parts\n",
    "        survival_rows.append({\n",
    "            'Time': cum_time,\n",
    "            'Event': row['Defect'],\n",
    "            'Shift': row['Shift'],\n",
    "            'Machine_Age_Cat': 'Young' if row['Machine_Age'] < 5 else 'Old'\n",
    "        })\n",
    "\n",
    "surv_df = pd.DataFrame(survival_rows)\n",
    "print(\n",
    "    f'Survival data: {len(surv_df)} observations, {surv_df[\"Event\"].sum()} defect events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b689226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4. KM curves stratified by shift and machine age ──\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# By shift\n",
    "shift_names = {0: 'Day', 1: 'Evening', 2: 'Night'}\n",
    "colors_shift = {0: '#4c72b0', 1: '#55a868', 2: '#c44e52'}\n",
    "for shift_id in [0, 1, 2]:\n",
    "    mask = surv_df['Shift'] == shift_id\n",
    "    t_km, S_km = kaplan_meier(surv_df.loc[mask, 'Time'].values,\n",
    "                              surv_df.loc[mask, 'Event'].values)\n",
    "    axes[0].step(t_km, S_km, where='post', lw=2, color=colors_shift[shift_id],\n",
    "                 label=f'{shift_names[shift_id]} (n={mask.sum()})')\n",
    "\n",
    "axes[0].set_title('Kaplan-Meier by Shift', fontsize=13)\n",
    "axes[0].set_xlabel('Time (min)')\n",
    "axes[0].set_ylabel('Survival Probability S(t)')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "\n",
    "# By machine age\n",
    "for age_cat, color in [('Young', '#4c72b0'), ('Old', '#c44e52')]:\n",
    "    mask = surv_df['Machine_Age_Cat'] == age_cat\n",
    "    t_km, S_km = kaplan_meier(surv_df.loc[mask, 'Time'].values,\n",
    "                              surv_df.loc[mask, 'Event'].values)\n",
    "    axes[1].step(t_km, S_km, where='post', lw=2, color=color,\n",
    "                 label=f'{age_cat} machine (n={mask.sum()})')\n",
    "\n",
    "axes[1].set_title('Kaplan-Meier by Machine Age', fontsize=13)\n",
    "axes[1].set_xlabel('Time (min)')\n",
    "axes[1].set_ylabel('Survival Probability S(t)')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "\n",
    "plt.suptitle('Survival Analysis — Time to Defect', fontsize=14, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Causal Inference — Propensity-Score Matching\n",
    "\n",
    "# We want to estimate the **causal effect** of night-shift operation on defect rate, controlling for confounders. Propensity-score matching pairs night-shift units with similar day-shift units.\n",
    "\n",
    "# ─── 5. Propensity-score matching ───────────────────────────\n",
    "# Treatment: Shift == 2 (night) vs Shift != 2 (control)\n",
    "df['is_night'] = (df['Shift'] == 2).astype(int)\n",
    "\n",
    "# Confounders: all process + quality vars except Shift\n",
    "CONFOUNDERS = [v for v in ALL_FEATS if v != 'Shift']\n",
    "scaler_ps = StandardScaler()\n",
    "X_conf = scaler_ps.fit_transform(df[CONFOUNDERS])\n",
    "\n",
    "# Logistic regression for propensity scores\n",
    "ps_model = LogisticRegression(max_iter=1000, C=1.0, random_state=0)\n",
    "ps_model.fit(X_conf, df['is_night'])\n",
    "propensity = ps_model.predict_proba(X_conf)[:, 1]\n",
    "\n",
    "df['propensity'] = propensity\n",
    "\n",
    "print(\n",
    "    f'Propensity score range: [{propensity.min():.3f}, {propensity.max():.3f}]')\n",
    "print(\n",
    "    f'Night-shift samples: {df[\"is_night\"].sum()}  |  Control: {(1-df[\"is_night\"]).sum()}')\n",
    "\n",
    "# Matching: for each treated unit, find the closest control unit (greedy)\n",
    "treated_idx = df.index[df['is_night'] == 1].tolist()\n",
    "control_idx = df.index[df['is_night'] == 0].tolist()\n",
    "control_ps = propensity[control_idx]\n",
    "\n",
    "matched_pairs = []\n",
    "used_controls = set()\n",
    "\n",
    "# Sort treated by propensity for efficiency\n",
    "treated_sorted = sorted(treated_idx, key=lambda i: propensity[i])\n",
    "\n",
    "for t_idx in treated_sorted:\n",
    "    t_ps = propensity[t_idx]\n",
    "    # Find closest unused control\n",
    "    best_c, best_diff = None, np.inf\n",
    "    for c_pos, c_idx in enumerate(control_idx):\n",
    "        if c_idx in used_controls:\n",
    "            continue\n",
    "        diff = abs(propensity[c_idx] - t_ps)\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_c = c_idx\n",
    "            best_c_pos = c_pos\n",
    "        if diff == 0:\n",
    "            break   # perfect match\n",
    "\n",
    "    if best_c is not None and best_diff < 0.05:  # caliper = 0.05\n",
    "        matched_pairs.append((t_idx, best_c))\n",
    "        used_controls.add(best_c)\n",
    "\n",
    "print(f'Matched pairs (caliper=0.05): {len(matched_pairs)}')\n",
    "\n",
    "# Estimate ATT (Average Treatment Effect on Treated)\n",
    "defect_treated = np.mean([df.loc[t, 'Defect'] for t, c in matched_pairs])\n",
    "defect_control = np.mean([df.loc[c, 'Defect'] for t, c in matched_pairs])\n",
    "ATT = defect_treated - defect_control\n",
    "\n",
    "print(f'\\nCausal Effect of Night Shift on Defect Rate:')\n",
    "print(f'  Matched treated defect rate:  {defect_treated*100:.2f}%')\n",
    "print(f'  Matched control defect rate:  {defect_control*100:.2f}%')\n",
    "print(f'  ATT (causal effect):          {ATT*100:+.2f} percentage points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67277b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6. Propensity + causal effect visualisation ──────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Propensity distribution before matching\n",
    "axes[0].hist(propensity[df['is_night'] == 1], bins=40, alpha=0.6, color='#c44e52',\n",
    "             density=True, label='Night shift')\n",
    "axes[0].hist(propensity[df['is_night'] == 0], bins=40, alpha=0.6, color='#4c72b0',\n",
    "             density=True, label='Control')\n",
    "axes[0].set_title('Propensity Score Distribution (Before Matching)')\n",
    "axes[0].set_xlabel('Propensity Score')\n",
    "axes[0].legend()\n",
    "\n",
    "# After matching\n",
    "matched_t_ps = [propensity[t] for t, c in matched_pairs]\n",
    "matched_c_ps = [propensity[c] for t, c in matched_pairs]\n",
    "axes[1].hist(matched_t_ps, bins=30, alpha=0.6, color='#c44e52',\n",
    "             density=True, label='Night (matched)')\n",
    "axes[1].hist(matched_c_ps, bins=30, alpha=0.6, color='#4c72b0',\n",
    "             density=True, label='Control (matched)')\n",
    "axes[1].set_title('Propensity Score Distribution (After Matching)')\n",
    "axes[1].set_xlabel('Propensity Score')\n",
    "axes[1].legend()\n",
    "\n",
    "# Treatment effect bar chart\n",
    "axes[2].bar(['Control\\n(matched)', 'Night Shift\\n(matched)', 'Causal Effect\\n(ATT)'],\n",
    "            [defect_control*100, defect_treated*100, ATT*100],\n",
    "            color=['#4c72b0', '#c44e52', '#8172b2'], edgecolor='white', width=0.5)\n",
    "axes[2].axhline(0, color='black', lw=0.5)\n",
    "axes[2].set_title('Causal Effect of Night Shift')\n",
    "axes[2].set_ylabel('Defect Rate (%)')\n",
    "for i, v in enumerate([defect_control*100, defect_treated*100, ATT*100]):\n",
    "    axes[2].text(i, v + 0.3, f'{v:+.1f}%',\n",
    "                 ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hand-Rolled Histogram Gradient Boosting (XGBoost-Style)\n",
    "\n",
    "# ─── 7. Histogram Gradient Boosting (simplified XGBoost) ──\n",
    "class HistGBTree:\n",
    "    \"\"\"Single decision stump using histogram-based splitting.\"\"\"\n",
    "\n",
    "    def __init__(self, n_bins=32):\n",
    "        self.n_bins = n_bins\n",
    "        self.split_feat = None\n",
    "        self.split_thresh = None\n",
    "        self.left_val = 0\n",
    "        self.right_val = 0\n",
    "\n",
    "    def fit(self, X, grad, hess, bin_edges):\n",
    "        \"\"\"Find the best split to minimise the second-order objective.\"\"\"\n",
    "        best_gain = -np.inf\n",
    "        n_feat = X.shape[1]\n",
    "\n",
    "        for feat in range(n_feat):\n",
    "            edges = bin_edges[feat]\n",
    "            for b in range(1, len(edges)-1):\n",
    "                thresh = edges[b]\n",
    "                left_mask = X[:, feat] <= thresh\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < 5 or right_mask.sum() < 5:\n",
    "                    continue\n",
    "\n",
    "                G_L = grad[left_mask].sum()\n",
    "                H_L = hess[left_mask].sum()\n",
    "                G_R = grad[right_mask].sum()\n",
    "                H_R = hess[right_mask].sum()\n",
    "\n",
    "                # Gain = 0.5 * (G_L²/H_L + G_R²/H_R - (G_L+G_R)²/(H_L+H_R))\n",
    "                gain = 0.5 * (G_L**2/(H_L+1e-6) + G_R**2/(H_R+1e-6)\n",
    "                              - (G_L+G_R)**2/(H_L+H_R+1e-6))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    self.split_feat = feat\n",
    "                    self.split_thresh = thresh\n",
    "                    self.left_val = -G_L / (H_L + 1e-6)\n",
    "                    self.right_val = -G_R / (H_R + 1e-6)\n",
    "\n",
    "    def predict(self, X):\n",
    "        mask = X[:, self.split_feat] <= self.split_thresh\n",
    "        preds = np.where(mask, self.left_val, self.right_val)\n",
    "        return preds\n",
    "\n",
    "\n",
    "class HandRolledGBM:\n",
    "    \"\"\"Binary classification via histogram gradient boosting.\"\"\"\n",
    "\n",
    "    def __init__(self, n_trees=200, learning_rate=0.1, n_bins=16):\n",
    "        self.n_trees = n_trees\n",
    "        self.lr = learning_rate\n",
    "        self.n_bins = n_bins\n",
    "        self.trees = []\n",
    "        self.bin_edges = None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -30, 30)))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Precompute bin edges per feature\n",
    "        self.bin_edges = []\n",
    "        for feat in range(X.shape[1]):\n",
    "            edges = np.quantile(X[:, feat], np.linspace(0, 1, self.n_bins+1))\n",
    "            self.bin_edges.append(edges)\n",
    "\n",
    "        F = np.zeros(len(y))   # cumulative prediction (log-odds)\n",
    "        for t in range(self.n_trees):\n",
    "            p = self._sigmoid(F)\n",
    "            grad = p - y              # gradient of log-loss\n",
    "            hess = p * (1 - p)        # hessian\n",
    "\n",
    "            tree = HistGBTree(self.n_bins)\n",
    "            tree.fit(X, grad, hess, self.bin_edges)\n",
    "            self.trees.append(tree)\n",
    "            F += self.lr * tree.predict(X)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        F = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            F += self.lr * tree.predict(X)\n",
    "        return self._sigmoid(F)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Train\n",
    "X_feat = df[ALL_FEATS].values\n",
    "y_feat = df['Defect'].values\n",
    "scaler_gb = StandardScaler()\n",
    "X_s = scaler_gb.fit_transform(X_feat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_s, y_feat, test_size=0.25,\n",
    "                                                    stratify=y_feat, random_state=42)\n",
    "\n",
    "print('Training Hand-Rolled GBM (200 trees) …')\n",
    "hgbm = HandRolledGBM(n_trees=200, learning_rate=0.08, n_bins=16)\n",
    "hgbm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "hgbm_proba = hgbm.predict_proba(X_test)\n",
    "hgbm_pred = hgbm.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, hgbm_proba)\n",
    "auc_val = auc(fpr, tpr)\n",
    "print(f'Hand-Rolled GBM AUC: {auc_val:.3f}')\n",
    "print(classification_report(y_test, hgbm_pred, target_names=['OK', 'Defect']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8. Compare with sklearn GBR ──────────────────────────\n",
    "sk_gbr = GradientBoostingClassifier(n_estimators=200, max_depth=2,\n",
    "                                    learning_rate=0.08, random_state=0)\n",
    "sk_gbr.fit(X_train, y_train)\n",
    "sk_proba = sk_gbr.predict_proba(X_test)[:, 1]\n",
    "fpr_sk, tpr_sk, _ = roc_curve(y_test, sk_proba)\n",
    "auc_sk = auc(fpr_sk, tpr_sk)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fpr, tpr, lw=2, color='steelblue',\n",
    "        label=f'Hand-Rolled GBM (AUC={auc_val:.3f})')\n",
    "ax.plot(fpr_sk, tpr_sk, lw=2, color='#c44e52',\n",
    "        label=f'Sklearn GBR (AUC={auc_sk:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=0.8)\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Comparison — Hand-Rolled vs Sklearn GBM')\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Shapley Root-Cause Attribution\n",
    "\n",
    "# ─── 9. Shapley values via marginal contribution sampling ─\n",
    "def shapley_values(model_fn, X_sample, X_background, n_perms=100, seed=0):\n",
    "    \"\"\"\n",
    "    Estimate Shapley values for one sample using sampling-based approach.\n",
    "    model_fn: callable X → probability\n",
    "    X_sample: single sample (1, d)\n",
    "    X_background: background dataset for marginalisation\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = X_sample.shape[1]\n",
    "    shap = np.zeros(d)\n",
    "    base = model_fn(X_background).mean()   # expected value under background\n",
    "\n",
    "    for _ in range(n_perms):\n",
    "        # Random permutation of features\n",
    "        perm = rng.permutation(d)\n",
    "        # Random background sample\n",
    "        bg = X_background[rng.integers(0, len(X_background))]\n",
    "        x_bg = bg.copy()\n",
    "        x_fg = bg.copy()\n",
    "\n",
    "        for feat in perm:\n",
    "            # Before including feat: x_bg has background for feat\n",
    "            pred_before = model_fn(x_bg.reshape(1, -1))[0]\n",
    "            # Include feat from sample\n",
    "            x_bg[feat] = X_sample[0, feat]\n",
    "            pred_after = model_fn(x_bg.reshape(1, -1))[0]\n",
    "            # Marginal contribution\n",
    "            shap[feat] += (pred_after - pred_before)\n",
    "\n",
    "    shap /= n_perms\n",
    "    return shap\n",
    "\n",
    "\n",
    "# Compute Shapley for top-10 defective samples\n",
    "defect_idx = np.where(y_test == 1)[0][:10]\n",
    "all_shap = []\n",
    "\n",
    "print('Computing Shapley values for 10 defective samples …')\n",
    "for i, idx in enumerate(defect_idx):\n",
    "    sv = shapley_values(\n",
    "        lambda X: sk_gbr.predict_proba(X)[:, 1],\n",
    "        X_test[idx:idx+1],\n",
    "        X_train[:500],   # subsample background for speed\n",
    "        n_perms=80, seed=i\n",
    "    )\n",
    "    all_shap.append(sv)\n",
    "    print(f'  Sample {i+1}/10 done.')\n",
    "\n",
    "shap_matrix = np.array(all_shap)   # (10, n_features)\n",
    "print('\\nShapley matrix shape:', shap_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29487449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10. Shapley beeswarm + waterfall plot ────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Beeswarm: mean |SHAP| across defective samples\n",
    "mean_abs_shap = np.abs(shap_matrix).mean(axis=0)\n",
    "ranked = np.argsort(mean_abs_shap)\n",
    "\n",
    "top_n = min(12, len(ALL_FEATS))\n",
    "top_idx = ranked[-top_n:]\n",
    "\n",
    "# Beeswarm-style dot plot\n",
    "for plot_pos, feat_idx in enumerate(top_idx):\n",
    "    vals = shap_matrix[:, feat_idx]\n",
    "    # Jitter y for visibility\n",
    "    jitter = np.random.default_rng(plot_pos).uniform(-0.15, 0.15, len(vals))\n",
    "    colors = ['#c44e52' if v > 0 else '#4c72b0' for v in vals]\n",
    "    axes[0].scatter(vals, np.full(len(vals), plot_pos) + jitter,\n",
    "                    c=colors, s=40, edgecolors='white', zorder=3)\n",
    "\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels([ALL_FEATS[i] for i in top_idx], fontsize=9)\n",
    "axes[0].axvline(0, color='black', lw=0.8)\n",
    "axes[0].set_xlabel('SHAP Value')\n",
    "axes[0].set_title('Shapley Beeswarm — Top Features (red=↑defect risk)')\n",
    "\n",
    "# Waterfall for the single highest-risk defective sample\n",
    "worst_idx = np.argmax(sk_gbr.predict_proba(X_test[defect_idx])[:, 1])\n",
    "worst_shap = shap_matrix[worst_idx]\n",
    "order = np.argsort(np.abs(worst_shap))[::-1][:10]\n",
    "cumulative = np.zeros(len(order)+1)\n",
    "base_val = sk_gbr.predict_proba(X_train[:500]).mean()   # E[f(x)]\n",
    "cumulative[0] = base_val\n",
    "for i, feat_idx in enumerate(order):\n",
    "    cumulative[i+1] = cumulative[i] + worst_shap[feat_idx]\n",
    "\n",
    "labels_wf = ['Base'] + [ALL_FEATS[i] for i in order]\n",
    "colors_wf = ['grey'] + ['#c44e52' if worst_shap[i]\n",
    "                        > 0 else '#4c72b0' for i in order]\n",
    "\n",
    "axes[1].bar(range(len(labels_wf)), cumulative,\n",
    "            color=colors_wf, edgecolor='white', width=0.6)\n",
    "axes[1].axhline(cumulative[-1], color='black', ls='--', lw=0.8, alpha=0.5)\n",
    "axes[1].set_xticks(range(len(labels_wf)))\n",
    "axes[1].set_xticklabels(labels_wf, rotation=30, ha='right', fontsize=8)\n",
    "axes[1].set_ylabel('Defect Probability')\n",
    "axes[1].set_title(f'Waterfall — Highest-Risk Sample (P={cumulative[-1]:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d102817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Capability Forecasting — Predicting Future Cpk\n",
    "\n",
    "# ─── 11. Rolling Cpk + forecasting ────────────────────────\n",
    "# Compute rolling Cpk for Wall_Thickness\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBReg\n",
    "USL_wt = 2.08   # upper spec limit\n",
    "LSL_wt = 1.92   # lower spec limit\n",
    "\n",
    "WIN_CPK = 200\n",
    "cpk_series = []\n",
    "for i in range(WIN_CPK, len(df), 50):\n",
    "    chunk = df['Wall_Thickness'].iloc[i-WIN_CPK:i]\n",
    "    mu = chunk.mean()\n",
    "    sig = chunk.std()\n",
    "    cpu = (USL_wt - mu) / (3*sig) if sig > 0 else 99\n",
    "    cpl = (mu - LSL_wt) / (3*sig) if sig > 0 else 99\n",
    "    cpk_series.append({'idx': i, 'Cpk': min(cpu, cpl), 'mu': mu, 'std': sig})\n",
    "\n",
    "cpk_df = pd.DataFrame(cpk_series)\n",
    "print(f'Cpk time series: {len(cpk_df)} points')\n",
    "print(f'  Mean Cpk: {cpk_df[\"Cpk\"].mean():.3f}')\n",
    "print(f'  Min Cpk:  {cpk_df[\"Cpk\"].min():.3f}')\n",
    "\n",
    "# Forecast Cpk using GBR on lagged Cpk values\n",
    "LAG_CPK = 5\n",
    "cpk_vals = cpk_df['Cpk'].values\n",
    "X_cpk = np.column_stack([cpk_vals[i:len(cpk_vals)-LAG_CPK+i]\n",
    "                        for i in range(LAG_CPK)])\n",
    "y_cpk = cpk_vals[LAG_CPK:]\n",
    "\n",
    "split_cpk = int(0.8 * len(X_cpk))\n",
    "cpk_model = GBReg(n_estimators=100, max_depth=3, random_state=0)\n",
    "cpk_model.fit(X_cpk[:split_cpk], y_cpk[:split_cpk])\n",
    "cpk_pred = cpk_model.predict(X_cpk[split_cpk:])\n",
    "\n",
    "cpk_rmse = np.sqrt(mean_squared_error(y_cpk[split_cpk:], cpk_pred))\n",
    "print(f'  Cpk Forecast RMSE: {cpk_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 12. Cpk forecast plot ─────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Full Cpk series\n",
    "ax.plot(cpk_df['idx'], cpk_df['Cpk'], lw=1.2,\n",
    "        color='steelblue', label='Actual Cpk')\n",
    "\n",
    "# Forecast region\n",
    "test_indices = cpk_df['idx'].values[LAG_CPK + split_cpk:]\n",
    "ax.plot(test_indices, cpk_pred, lw=1.5, color='crimson',\n",
    "        ls='--', label='Forecasted Cpk')\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(1.33, color='green', ls=':', lw=1, label='Target (1.33)')\n",
    "ax.axhline(1.00, color='orange', ls=':', lw=1, label='Minimum (1.00)')\n",
    "\n",
    "# Shade forecast region\n",
    "ax.axvspan(test_indices[0], test_indices[-1], alpha=0.08,\n",
    "           color='crimson', label='Forecast window')\n",
    "\n",
    "ax.set_title('Process Capability (Cpk) — Actual vs Forecasted', fontsize=13)\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Cpk')\n",
    "ax.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cost-Benefit Analysis — Optimal Inspection Policy\n",
    "\n",
    "# ─── 13. Expected cost model ──────────────────────────────\n",
    "# Cost parameters (arbitrary but realistic units)\n",
    "COST_DEFECT = 500   # cost of a defect reaching customer ($)\n",
    "COST_INSPECTION = 25    # cost of inspecting one part ($)\n",
    "COST_SCRAP = 80    # cost of scrapping a detected defect ($)\n",
    "\n",
    "# For each test sample, compute expected cost under two policies:\n",
    "# Policy A: Ship without inspection\n",
    "# Policy B: Inspect (if defect found → scrap; if not → ship)\n",
    "\n",
    "proba_defect = sk_gbr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# expected cost if shipped\n",
    "cost_no_inspect = proba_defect * COST_DEFECT\n",
    "cost_inspect = COST_INSPECTION + proba_defect * \\\n",
    "    COST_SCRAP             # expected cost if inspected\n",
    "\n",
    "# Optimal policy: inspect if cost_inspect < cost_no_inspect\n",
    "# → inspect when p > COST_INSPECTION / (COST_DEFECT - COST_SCRAP)\n",
    "threshold_optimal = COST_INSPECTION / (COST_DEFECT - COST_SCRAP)\n",
    "print(f'Optimal inspection threshold: p > {threshold_optimal:.4f}')\n",
    "print(\n",
    "    f'  (i.e., inspect when predicted defect prob > {threshold_optimal*100:.2f}%)')\n",
    "\n",
    "optimal_policy = (proba_defect > threshold_optimal).astype(int)\n",
    "actual_cost = np.where(optimal_policy == 1,\n",
    "                       COST_INSPECTION + y_test * COST_SCRAP,\n",
    "                       y_test * COST_DEFECT)\n",
    "naive_cost = y_test * COST_DEFECT   # ship everything\n",
    "\n",
    "total_optimal = actual_cost.sum()\n",
    "total_naive = naive_cost.sum()\n",
    "savings = total_naive - total_optimal\n",
    "print(f'\\n  Total cost (ship all):      ${total_naive:,.0f}')\n",
    "print(f'  Total cost (optimal policy): ${total_optimal:,.0f}')\n",
    "print(\n",
    "    f'  Savings:                     ${savings:,.0f} ({savings/total_naive*100:.1f}%)')\n",
    "print(\n",
    "    f'  Parts inspected:            {optimal_policy.sum()} / {len(optimal_policy)} ({optimal_policy.mean()*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a31884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 14. Cost analysis visualisation ──────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Cost distribution comparison\n",
    "thresholds = np.linspace(0, 0.5, 100)\n",
    "total_costs = []\n",
    "for t in thresholds:\n",
    "    policy = (proba_defect > t).astype(int)\n",
    "    cost = np.where(policy == 1,\n",
    "                    COST_INSPECTION + y_test * COST_SCRAP,\n",
    "                    y_test * COST_DEFECT).sum()\n",
    "    total_costs.append(cost)\n",
    "\n",
    "axes[0].plot(thresholds, total_costs, lw=2, color='steelblue')\n",
    "axes[0].axvline(threshold_optimal, color='crimson', ls='--', lw=1.5,\n",
    "                label=f'Optimal threshold={threshold_optimal:.3f}')\n",
    "axes[0].set_title('Total Cost vs Inspection Threshold')\n",
    "axes[0].set_xlabel('Defect Probability Threshold')\n",
    "axes[0].set_ylabel('Total Expected Cost ($)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Breakdown: inspected vs shipped\n",
    "categories = ['Ship All\\n(naive)', 'Optimal\\nPolicy']\n",
    "defects_escaped = [y_test.sum(), ((1-optimal_policy) * y_test).sum()]\n",
    "defects_caught = [0, (optimal_policy * y_test).sum()]\n",
    "no_defect = [len(y_test)-y_test.sum(), len(y_test)-y_test.sum()]\n",
    "\n",
    "x_pos = [0, 1]\n",
    "axes[1].bar(x_pos, defects_escaped, color='#c44e52', label='Defects escaped')\n",
    "axes[1].bar(x_pos, defects_caught, bottom=defects_escaped,\n",
    "            color='#55a868', label='Defects caught')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(categories)\n",
    "axes[1].set_title('Defect Disposition by Policy')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "# Cost per part distribution\n",
    "cost_per_part_naive = y_test * COST_DEFECT\n",
    "cost_per_part_optimal = np.where(optimal_policy == 1,\n",
    "                                 COST_INSPECTION + y_test * COST_SCRAP,\n",
    "                                 y_test * COST_DEFECT)\n",
    "axes[2].hist(cost_per_part_naive[cost_per_part_naive > 0], bins=20, alpha=0.5,\n",
    "             color='#c44e52', label='Naive', density=True)\n",
    "axes[2].hist(cost_per_part_optimal[cost_per_part_optimal > 0], bins=20, alpha=0.5,\n",
    "             color='#4c72b0', label='Optimal', density=True)\n",
    "axes[2].set_title('Per-Part Cost Distribution (non-zero only)')\n",
    "axes[2].set_xlabel('Cost ($)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Cost-Benefit Analysis — Optimal Inspection Policy',\n",
    "             fontsize=14, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a446f",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Portfolio Takeaways\n",
    "\n",
    "| Technique | Value |\n",
    "|---|---|\n",
    "| **Kaplan-Meier** | Nonparametric survival curves reveal that night shift and old machines degrade faster |\n",
    "| **Propensity-Score Matching** | Isolates the **causal** effect of night shift on defect rate — not just correlation |\n",
    "| **Hand-Rolled GBM** | Demonstrates deep understanding of boosting internals — histogram splits, second-order gradients |\n",
    "| **Shapley Attribution** | Beeswarm + waterfall plots provide actionable root-cause explanations per defective part |\n",
    "| **Cpk Forecasting** | Predicts future capability drift — enables **proactive** maintenance before quality degrades |\n",
    "| **Cost-Benefit Policy** | Translates model output into an **optimal business decision** — the final step from ML to value |\n",
    "\n",
    "This notebook completes the full quality analytics pipeline: **monitor → diagnose → explain → decide → forecast**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
