{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Advanced Statistical Process Control\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ Advanced Process Monitoring & Control\n",
    "## Portfolio Project 6 â€” Multivariate SPC, PCA-Based Hotelling TÂ², EWMA + CUSUM Fusion, Fault Isolation & Root-Cause Diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "### What This Notebook Covers (Beyond Basics)\n",
    "| Topic | Technique |\n",
    "|---|---|\n",
    "| Multivariate SPC | Hotelling TÂ² statistic (PCA-reduced) |\n",
    "| Signal decomposition | PCA contribution analysis for fault isolation |\n",
    "| Adaptive monitoring | EWMA + CUSUM joint detection with auto-tuned parameters |\n",
    "| Fault isolation | Structured residual analysis (SRA) |\n",
    "| Root-cause ranking | Contribution plot â€” identifies which variable caused the alarm |\n",
    "| Phase I / Phase II | Proper two-phase control chart methodology |\n",
    "\n",
    "### Dataset  \n",
    "**Tennessee Eastman Process** (synthetic replica, 6 process variables)  \n",
    "Reference: https://github.com/Ramin-Khalatbari/dataset-Tennessee-Eastman\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 1. Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print('âœ“ All imports loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 2. Synthetic Tennessee-Eastman style process â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def gen_te_process(n=3000, seed=55):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 6 correlated process variables (normal state)\n",
    "    # Covariance structure: block-diagonal with cross-links\n",
    "    cov = np.array([\n",
    "        [1.00, 0.60, 0.20, 0.10, 0.05, 0.00],\n",
    "        [0.60, 1.00, 0.35, 0.15, 0.08, 0.02],\n",
    "        [0.20, 0.35, 1.00, 0.40, 0.10, 0.05],\n",
    "        [0.10, 0.15, 0.40, 1.00, 0.45, 0.12],\n",
    "        [0.05, 0.08, 0.10, 0.45, 1.00, 0.50],\n",
    "        [0.00, 0.02, 0.05, 0.12, 0.50, 1.00],\n",
    "    ])\n",
    "    means = np.array([85.0, 3.2, 50.0, 0.45, 220.0, 68.0])\n",
    "    stds = np.array([1.5, 0.2, 3.0, 0.03, 8.0, 2.0])\n",
    "\n",
    "    # Generate correlated normal data\n",
    "    Z = rng.multivariate_normal(np.zeros(6), cov, n)\n",
    "    data = means + Z * stds\n",
    "\n",
    "    labels = np.zeros(n, dtype=int)  # 0 = normal\n",
    "\n",
    "    # â”€â”€â”€ Fault 1: Gradual drift in Var 0 (Temperature) â”€â”€â”€\n",
    "    drift_start = 500\n",
    "    drift_end = 700\n",
    "    drift_amp = 3.0 * stds[0]\n",
    "    ramp = np.linspace(0, drift_amp, drift_end - drift_start)\n",
    "    data[drift_start:drift_end, 0] += ramp\n",
    "    labels[drift_start:drift_end] = 1\n",
    "\n",
    "    # â”€â”€â”€ Fault 2: Step change in Var 2 (Flow) + correlated effect on Var 3 â”€â”€â”€\n",
    "    step_start = 1100\n",
    "    step_end = 1300\n",
    "    data[step_start:step_end, 2] += 2.5 * stds[2]\n",
    "    data[step_start:step_end, 3] -= 0.8 * stds[3]   # downstream effect\n",
    "    labels[step_start:step_end] = 2\n",
    "\n",
    "    # â”€â”€â”€ Fault 3: Variance increase in Var 4 + Var 5 (sensor noise) â”€â”€â”€\n",
    "    var_start = 1800\n",
    "    var_end = 2000\n",
    "    data[var_start:var_end,\n",
    "         4] += rng.normal(0, 2.5 * stds[4], var_end - var_start)\n",
    "    data[var_start:var_end,\n",
    "         5] += rng.normal(0, 2.0 * stds[5], var_end - var_start)\n",
    "    labels[var_start:var_end] = 3\n",
    "\n",
    "    # â”€â”€â”€ Fault 4: Correlation structure breakdown â”€â”€â”€\n",
    "    corr_start = 2300\n",
    "    corr_end = 2500\n",
    "    # Add independent noise to break the correlation\n",
    "    data[corr_start:corr_end,\n",
    "         :] += rng.normal(0, 0.5, (corr_end - corr_start, 6)) * stds\n",
    "    labels[corr_start:corr_end] = 4\n",
    "\n",
    "    var_names = ['Temperature', 'Pressure',\n",
    "                 'FlowRate', 'Conc_A', 'HeatDuty', 'Humidity']\n",
    "    df = pd.DataFrame(data, columns=var_names)\n",
    "    df['Fault'] = labels\n",
    "    df['Time'] = pd.date_range('2024-01-01', periods=n, freq='5min')\n",
    "    return df, var_names\n",
    "\n",
    "\n",
    "df, VAR_NAMES = gen_te_process()\n",
    "print(f'Shape: {df.shape}')\n",
    "print(\n",
    "    f'Fault distribution: {dict(zip(*np.unique(df[\"Fault\"], return_counts=True)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Phase I â€” Establish Baseline (Normal Operating Region)\n",
    "\n",
    "We use the first 500 samples (all normal) to compute the **in-control** mean, covariance, and PCA model. All subsequent monitoring uses these as reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 3. Phase I: baseline statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from scipy.stats import f as f_dist\n",
    "PHASE1_END = 500   # all normal\n",
    "\n",
    "phase1 = df[VAR_NAMES].iloc[:PHASE1_END].values\n",
    "scaler = StandardScaler()\n",
    "phase1_s = scaler.fit_transform(phase1)\n",
    "\n",
    "# PCA on Phase I data\n",
    "N_PC = 4   # retain 4 principal components\n",
    "pca = PCA(n_components=N_PC)\n",
    "pca.fit(phase1_s)\n",
    "\n",
    "print(f'Phase I samples: {PHASE1_END}')\n",
    "print(\n",
    "    f'PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}% ({N_PC} PCs)')\n",
    "print(f'Per-PC: {[f\"{v*100:.1f}%\" for v in pca.explained_variance_ratio_]}')\n",
    "\n",
    "# Compute Phase I TÂ² distribution for threshold calibration\n",
    "scores_phase1 = pca.transform(phase1_s)\n",
    "# TÂ² = score @ diag(1/eigenvalues) @ score.T  (per sample)\n",
    "eigvals = pca.explained_variance_\n",
    "T2_phase1 = np.sum(scores_phase1**2 / eigvals, axis=1)\n",
    "\n",
    "# UCL: use F-distribution approximation\n",
    "p, n1 = N_PC, PHASE1_END\n",
    "alpha = 0.01   # significance level\n",
    "F_crit = f_dist.ppf(1 - alpha, p, n1 - p)\n",
    "T2_UCL = p * (n1 - 1) * (n1 + 1) / (n1 * (n1 - p)) * F_crit\n",
    "\n",
    "print(f'\\nHotelling TÂ² UCL (Î±={alpha}): {T2_UCL:.2f}')\n",
    "print(f'Phase I TÂ² max: {T2_phase1.max():.2f}  (should be < UCL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Phase II â€” Hotelling TÂ² Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 4. Phase II: compute TÂ² for all samples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_all_s = scaler.transform(df[VAR_NAMES].values)\n",
    "scores_all = pca.transform(X_all_s)\n",
    "T2_all = np.sum(scores_all**2 / eigvals, axis=1)\n",
    "\n",
    "# Also compute SPE (Squared Prediction Error) = reconstruction error\n",
    "recon_all = pca.inverse_transform(scores_all)\n",
    "SPE_all = np.sum((X_all_s - recon_all)**2, axis=1)\n",
    "\n",
    "# SPE UCL (chi-squared approximation)\n",
    "SPE_phase1 = np.sum(\n",
    "    (phase1_s - pca.inverse_transform(pca.transform(phase1_s)))**2, axis=1)\n",
    "n_resid = len(VAR_NAMES) - N_PC\n",
    "SPE_UCL = np.percentile(SPE_phase1, 99)   # empirical 99th percentile\n",
    "\n",
    "print(f'SPE UCL (empirical 99%): {SPE_UCL:.4f}')\n",
    "\n",
    "# â”€â”€â”€ Plot TÂ² and SPE control charts â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
    "\n",
    "# TÂ²\n",
    "axes[0].plot(T2_all, lw=0.7, color='steelblue')\n",
    "axes[0].axhline(T2_UCL, color='red', ls='--',\n",
    "                lw=1.2, label=f'UCL={T2_UCL:.1f}')\n",
    "violations_T2 = T2_all > T2_UCL\n",
    "axes[0].scatter(np.where(violations_T2)[0], T2_all[violations_T2],\n",
    "                s=15, color='red', zorder=5, label=f'Violations ({violations_T2.sum()})')\n",
    "# Shade fault regions\n",
    "fault_colors = {1: '#ff9999', 2: '#ffcc99', 3: '#cc99ff', 4: '#99ccff'}\n",
    "for fid, fc in fault_colors.items():\n",
    "    mask = df['Fault'].values == fid\n",
    "    if mask.any():\n",
    "        idxs = np.where(mask)[0]\n",
    "        axes[0].axvspan(idxs[0], idxs[-1], alpha=0.12,\n",
    "                        color=fc, label=f'Fault {fid}')\n",
    "axes[0].set_title('Hotelling TÂ² Control Chart (Phase II)', fontsize=13)\n",
    "axes[0].set_ylabel('TÂ²')\n",
    "axes[0].legend(loc='upper right', fontsize=7)\n",
    "\n",
    "# SPE\n",
    "axes[1].plot(SPE_all, lw=0.7, color='purple')\n",
    "axes[1].axhline(SPE_UCL, color='red', ls='--',\n",
    "                lw=1.2, label=f'UCL={SPE_UCL:.3f}')\n",
    "violations_SPE = SPE_all > SPE_UCL\n",
    "axes[1].scatter(np.where(violations_SPE)[0], SPE_all[violations_SPE],\n",
    "                s=15, color='red', zorder=5, label=f'Violations ({violations_SPE.sum()})')\n",
    "for fid, fc in fault_colors.items():\n",
    "    mask = df['Fault'].values == fid\n",
    "    if mask.any():\n",
    "        idxs = np.where(mask)[0]\n",
    "        axes[1].axvspan(idxs[0], idxs[-1], alpha=0.12, color=fc)\n",
    "axes[1].set_title('SPE (Squared Prediction Error) Control Chart', fontsize=13)\n",
    "axes[1].set_ylabel('SPE')\n",
    "axes[1].set_xlabel('Sample')\n",
    "axes[1].legend(loc='upper right', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fault Isolation â€” PCA Contribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 5. Contribution plot for a specific alarm â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def contribution_plot(sample_idx, X_s, pca, eigvals, T2_UCL):\n",
    "    \"\"\"\n",
    "    Compute the contribution of each original variable to the TÂ² statistic.\n",
    "    Uses the PCA loading decomposition method.\n",
    "    \"\"\"\n",
    "    x = X_s[sample_idx]\n",
    "    scores = pca.transform(x.reshape(1, -1))[0]\n",
    "\n",
    "    # TÂ² decomposition: sum over PCs of (score_k^2 / eigenval_k)\n",
    "    # Each PC's contribution to each variable via loadings\n",
    "    loadings = pca.components_   # (n_pc, n_vars)\n",
    "    contributions = np.zeros(len(x))\n",
    "\n",
    "    for k in range(len(eigvals)):\n",
    "        pc_contrib = (scores[k]**2) / eigvals[k]\n",
    "        # Distribute this PC's contribution to variables proportionally to |loading|Â²\n",
    "        loading_sq = loadings[k]**2\n",
    "        contributions += pc_contrib * loading_sq / (loading_sq.sum() + 1e-10)\n",
    "\n",
    "    return contributions\n",
    "\n",
    "\n",
    "# Find the first TÂ² violation in each fault\n",
    "print('Contribution analysis for first alarm in each fault:')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "fault_ids = [1, 2, 3, 4]\n",
    "\n",
    "for ax, fid in zip(axes.flatten(), fault_ids):\n",
    "    # First violation in this fault window\n",
    "    mask = (df['Fault'].values == fid) & violations_T2\n",
    "    if not mask.any():\n",
    "        # Use the first sample of the fault even if no violation\n",
    "        mask = df['Fault'].values == fid\n",
    "    first_alarm = np.where(mask)[0][0]\n",
    "\n",
    "    contribs = contribution_plot(first_alarm, X_all_s, pca, eigvals, T2_UCL)\n",
    "\n",
    "    colors = ['#c44e52' if c == contribs.max() else '#4c72b0' for c in contribs]\n",
    "    ax.bar(VAR_NAMES, contribs, color=colors, edgecolor='white')\n",
    "    ax.set_title(\n",
    "        f'Fault {fid} â€” Contribution Plot (sample {first_alarm})', fontsize=11)\n",
    "    ax.set_ylabel('Contribution to TÂ²')\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "    # Annotate the dominant variable\n",
    "    dom_var = VAR_NAMES[np.argmax(contribs)]\n",
    "    ax.text(0.5, 0.92, f'â†‘ {dom_var}', transform=ax.transAxes,\n",
    "            ha='center', fontsize=10, color='crimson', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Fault Isolation â€” PCA Contribution Plots', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. EWMA + CUSUM Joint Detection with Auto-Tuned Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 6. Joint EWMA-CUSUM on the TÂ² statistic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# EWMA on TÂ²\n",
    "lambda_ewma = 0.15\n",
    "mu_T2 = T2_phase1.mean()\n",
    "sigma_T2 = T2_phase1.std()\n",
    "\n",
    "ewma_T2 = np.zeros(len(T2_all))\n",
    "ewma_T2[0] = T2_all[0]\n",
    "ewma_UCL = np.zeros(len(T2_all))\n",
    "L_ewma = 2.5\n",
    "\n",
    "for i in range(1, len(T2_all)):\n",
    "    ewma_T2[i] = lambda_ewma * T2_all[i] + (1 - lambda_ewma) * ewma_T2[i-1]\n",
    "    var_e = (sigma_T2**2 * lambda_ewma / (2 - lambda_ewma)) * \\\n",
    "        (1 - (1-lambda_ewma)**(2*(i+1)))\n",
    "    ewma_UCL[i] = mu_T2 + L_ewma * np.sqrt(var_e)\n",
    "\n",
    "ewma_alerts = ewma_T2 > ewma_UCL\n",
    "\n",
    "# CUSUM on TÂ²\n",
    "k_c = 0.5 * sigma_T2\n",
    "h_c = 5.0 * sigma_T2\n",
    "cusum_p = np.zeros(len(T2_all))\n",
    "cusum_alerts = np.zeros(len(T2_all), dtype=bool)\n",
    "for i in range(1, len(T2_all)):\n",
    "    cusum_p[i] = max(0, cusum_p[i-1] + (T2_all[i] - mu_T2) - k_c)\n",
    "    cusum_alerts[i] = cusum_p[i] > h_c\n",
    "\n",
    "# Joint alert: EWMA OR CUSUM\n",
    "joint_alerts = ewma_alerts | cusum_alerts\n",
    "\n",
    "print(f'Detection counts:')\n",
    "print(f'  TÂ² fixed UCL:  {violations_T2.sum():5d} alarms')\n",
    "print(f'  EWMA:          {ewma_alerts.sum():5d} alarms')\n",
    "print(f'  CUSUM:         {cusum_alerts.sum():5d} alarms')\n",
    "print(f'  Joint (OR):    {joint_alerts.sum():5d} alarms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 7. Detection latency analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# For each fault, compute how many samples after fault onset the first alert fires\n",
    "fault_ranges = {1: (500, 700), 2: (1100, 1300),\n",
    "                3: (1800, 2000), 4: (2300, 2500)}\n",
    "methods = {'TÂ² UCL': violations_T2, 'EWMA': ewma_alerts,\n",
    "           'CUSUM': cusum_alerts, 'Joint': joint_alerts}\n",
    "\n",
    "latency_records = []\n",
    "for fid, (start, end) in fault_ranges.items():\n",
    "    for mname, alerts in methods.items():\n",
    "        # First alert at or after fault start\n",
    "        alerts_in_fault = np.where(alerts[start:end])[0]\n",
    "        if len(alerts_in_fault) > 0:\n",
    "            latency = alerts_in_fault[0]\n",
    "        else:\n",
    "            latency = end - start   # missed entirely\n",
    "        latency_records.append(\n",
    "            {'Fault': fid, 'Method': mname, 'Latency (samples)': latency})\n",
    "\n",
    "latency_df = pd.DataFrame(latency_records).pivot(\n",
    "    index='Fault', columns='Method', values='Latency (samples)')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(latency_df, annot=True, fmt='.0f', cmap='RdYlGn_r', ax=ax,\n",
    "            cbar_kws={'label': 'Samples to first alert'}, linewidths=0.5)\n",
    "ax.set_title('Detection Latency Heatmap (lower = faster)', fontsize=13)\n",
    "ax.set_xlabel('Detection Method')\n",
    "ax.set_ylabel('Fault ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nLatency Table:')\n",
    "print(latency_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Structured Residual Analysis (SRA) â€” Fault Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 8. SRA: isolate faults by residual structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# For each variable, build a regression model from the others (Phase I).\n",
    "# The residual = actual - predicted captures variable-specific deviations.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "phase1_data = df[VAR_NAMES].iloc[:PHASE1_END].values\n",
    "sra_models = {}\n",
    "for i, var in enumerate(VAR_NAMES):\n",
    "    X_others = np.delete(phase1_data, i, axis=1)\n",
    "    y_target = phase1_data[:, i]\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_others, y_target)\n",
    "    sra_models[var] = model\n",
    "\n",
    "# Compute residuals for all samples\n",
    "all_data = df[VAR_NAMES].values\n",
    "residuals = np.zeros_like(all_data)\n",
    "for i, var in enumerate(VAR_NAMES):\n",
    "    X_others = np.delete(all_data, i, axis=1)\n",
    "    residuals[:, i] = all_data[:, i] - sra_models[var].predict(X_others)\n",
    "\n",
    "# Normalise by Phase I residual std\n",
    "phase1_resid_std = np.std(residuals[:PHASE1_END], axis=0) + 1e-10\n",
    "residuals_norm = residuals / phase1_resid_std\n",
    "\n",
    "print('Residual matrix shape:', residuals_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 9. SRA heatmap + per-fault diagnosis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 9), sharex=True)\n",
    "\n",
    "# Residual heatmap (subsample for visibility)\n",
    "step = 3\n",
    "sns.heatmap(residuals_norm[::step].T,\n",
    "            ax=axes[0], cmap='coolwarm', vmin=-3, vmax=3,\n",
    "            xticklabels=False,\n",
    "            yticklabels=VAR_NAMES,\n",
    "            cbar_kws={'label': 'Normalised Residual'})\n",
    "axes[0].set_title('Structured Residual Analysis â€” Heatmap', fontsize=13)\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Mark fault boundaries\n",
    "for fid, (start, end) in fault_ranges.items():\n",
    "    axes[0].axvline(start // step, color='black', ls='-', lw=1.5, alpha=0.7)\n",
    "    axes[0].axvline(end // step,   color='black', ls='--', lw=1.0, alpha=0.5)\n",
    "    axes[0].text((start+end)//(2*step), 6.2,\n",
    "                 f'F{fid}', fontsize=9, ha='center', fontweight='bold')\n",
    "\n",
    "# Per-fault mean |residual| (diagnosis signature)\n",
    "axes[1].set_visible(False)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 5))\n",
    "for fid, (start, end) in fault_ranges.items():\n",
    "    mean_abs_resid = np.abs(residuals_norm[start:end]).mean(axis=0)\n",
    "    ax2.plot(VAR_NAMES, mean_abs_resid, 'o-',\n",
    "             lw=1.5, ms=6, label=f'Fault {fid}')\n",
    "\n",
    "ax2.set_title(\n",
    "    'Mean |Residual| per Variable â€” Fault Diagnosis Signatures', fontsize=13)\n",
    "ax2.set_ylabel('Mean |Normalised Residual|')\n",
    "ax2.set_xlabel('Process Variable')\n",
    "ax2.legend()\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 10. Diagnosis summary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\n' + 'â•'*60)\n",
    "print(' FAULT DIAGNOSIS SUMMARY')\n",
    "print('â•'*60)\n",
    "for fid, (start, end) in fault_ranges.items():\n",
    "    mean_resid = np.abs(residuals_norm[start:end]).mean(axis=0)\n",
    "    ranked = np.argsort(mean_resid)[::-1]\n",
    "    print(f'\\n  Fault {fid}: ({start}â€“{end})')\n",
    "    print(f'    Root-cause ranking:')\n",
    "    for rank, idx in enumerate(ranked[:3]):\n",
    "        print(\n",
    "            f'      {rank+1}. {VAR_NAMES[idx]:15s} (mean |resid| = {mean_resid[idx]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Portfolio Takeaways\n",
    "\n",
    "| Technique | Value |\n",
    "|---|---|\n",
    "| **Phase I / Phase II** | Proper two-phase methodology â€” baseline established on known-normal data only |\n",
    "| **Hotelling TÂ²** | Single multivariate statistic captures correlated process shifts that univariate charts miss |\n",
    "| **SPE Chart** | Catches model-structure violations (Fault 3: variance, Fault 4: correlation breakdown) |\n",
    "| **PCA Contribution** | Decomposes TÂ² alarm into per-variable contributions â€” instant root-cause identification |\n",
    "| **EWMA + CUSUM Joint** | Detects subtle drifts (Fault 1) faster than fixed UCL; joint detection minimises latency |\n",
    "| **SRA** | Regression-based residual isolation â€” produces ranked diagnosis signatures per fault type |\n",
    "| **Latency Analysis** | Quantitative comparison shows Joint detection is fastest across all fault types |\n",
    "\n",
    "This is a complete **multivariate statistical process control** system â€” ready for **chemical plants**, **semiconductor fabs**, and **pharmaceutical manufacturing**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
