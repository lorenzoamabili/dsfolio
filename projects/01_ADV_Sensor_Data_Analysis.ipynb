{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Advanced Sensor Data Analysis\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1. Imports ─────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import welch, butter, filtfilt\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print('✓ All imports loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2. Synthetic 16-channel gas-sensor array ────────────────\n",
    "def make_sensor_data(n=20000, n_ch=16, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    t = np.linspace(0, 200, n)\n",
    "\n",
    "    # Each sensor: sum of 2-3 sinusoids + correlated drift + noise\n",
    "    base_freqs = rng.uniform(0.05, 0.8, (n_ch, 3))   # Hz-equivalent\n",
    "    base_amps = rng.uniform(0.2, 1.5, (n_ch, 3))\n",
    "    phases = rng.uniform(0, 2*np.pi, (n_ch, 3))\n",
    "\n",
    "    signals = np.zeros((n, n_ch))\n",
    "    for ch in range(n_ch):\n",
    "        for k in range(3):\n",
    "            signals[:, ch] += base_amps[ch, k] * np.sin(\n",
    "                2*np.pi*base_freqs[ch, k]*t + phases[ch, k])\n",
    "\n",
    "    # Shared low-freq drift (environmental)\n",
    "    env_drift = np.sin(t / 40) * 0.4 + np.sin(t / 17 + 1) * 0.2\n",
    "    signals += env_drift[:, None]\n",
    "\n",
    "    # Cross-correlated noise (sensor clusters 0-3, 4-7, 8-11, 12-15)\n",
    "    for cluster_start in range(0, n_ch, 4):\n",
    "        shared = rng.normal(0, 0.12, n)\n",
    "        signals[:, cluster_start:cluster_start+4] += shared[:, None]\n",
    "\n",
    "    # Independent noise\n",
    "    signals += rng.normal(0, 0.06, (n, n_ch))\n",
    "\n",
    "    # Inject transient events (gas pulses) — correlated across cluster\n",
    "    for _ in range(40):\n",
    "        idx = rng.integers(100, n-100)\n",
    "        clust = rng.integers(0, 4)\n",
    "        width = rng.integers(30, 120)\n",
    "        amp = rng.uniform(0.8, 2.5)\n",
    "        pulse = amp * \\\n",
    "            np.exp(-0.5 * ((np.arange(width) - width//2) / (width/6))**2)\n",
    "        end = min(idx + width, n)\n",
    "        pw = end - idx\n",
    "        for ch in range(clust*4, clust*4+4):\n",
    "            signals[idx:end, ch] += pulse[:pw] * rng.uniform(0.5, 1.0)\n",
    "\n",
    "    cols = [f'Sensor_{i:02d}' for i in range(n_ch)]\n",
    "    df = pd.DataFrame(signals, columns=cols)\n",
    "    df['Time'] = pd.date_range('2024-01-01', periods=n, freq='1min')\n",
    "    return df\n",
    "\n",
    "\n",
    "df = make_sensor_data()\n",
    "print(f'Shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 1. Power Spectral Density (PSD) — Welch's Method\n",
    "\n",
    "Welch's method segments the signal, windows each segment, computes per-segment FFTs, and averages — reducing spectral leakage and variance compared to a raw periodogram. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3. PSD via Welch for every sensor ──────────────────────\n",
    "sensor_cols = [c for c in df.columns if c.startswith('Sensor')]\n",
    "fs = 1.0  # 1 sample / min\n",
    "\n",
    "psd_matrix = {}   # sensor → (freqs, Pxx)\n",
    "for col in sensor_cols:\n",
    "    f, Pxx = welch(df[col].values, fs=fs, nperseg=512, noverlap=256,\n",
    "                   detrend='linear', scaling='density')\n",
    "    psd_matrix[col] = (f, Pxx)\n",
    "\n",
    "# Plot first 8 sensors\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 7), sharex=True, sharey=False)\n",
    "for ax, col in zip(axes.flatten(), sensor_cols[:8]):\n",
    "    f, Pxx = psd_matrix[col]\n",
    "    ax.semilogy(f, Pxx, lw=0.9, color='steelblue')\n",
    "    ax.set_title(col, fontsize=10)\n",
    "    ax.set_xlabel('Frequency (cycles/min)')\n",
    "    ax.set_ylabel('PSD')\n",
    "    # Mark dominant frequency (exclude DC)\n",
    "    mask = f > 0.01\n",
    "    dom_idx = np.argmax(Pxx[mask])\n",
    "    dom_f = f[mask][dom_idx]\n",
    "    ax.axvline(dom_f, color='crimson', ls='--', lw=0.8, alpha=0.7)\n",
    "    ax.text(dom_f+0.005, Pxx[mask][dom_idx]*0.5,\n",
    "            f'{dom_f:.3f}', fontsize=7, color='crimson')\n",
    "\n",
    "plt.suptitle('Power Spectral Density — Welch (8 Sensors)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4. Dominant-frequency heatmap across all 16 sensors ──\n",
    "dom_freqs = {}\n",
    "for col in sensor_cols:\n",
    "    f, Pxx = psd_matrix[col]\n",
    "    mask = f > 0.01\n",
    "    dom_freqs[col] = f[mask][np.argmax(Pxx[mask])]\n",
    "\n",
    "freq_df = pd.DataFrame(\n",
    "    {'Sensor': sensor_cols, 'Dominant Freq (cycles/min)': list(dom_freqs.values())})\n",
    "freq_df = freq_df.set_index('Sensor').sort_values(\n",
    "    'Dominant Freq (cycles/min)', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(freq_df, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            ax=ax, cbar_kws={'label': 'Freq'})\n",
    "ax.set_title('Dominant Frequency per Sensor', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 2. Discrete Wavelet Transform — Time–Frequency Energy Map\n",
    "\n",
    "DWT decomposes a signal into approximation (low-freq) and detail (high-freq) coefficients at multiple levels. We compute the **energy** at each level across sliding windows to build a 2-D time–frequency energy map. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5. Manual DWT (Haar) — no pywt dependency ─────────────\n",
    "def haar_dwt_1level(x):\n",
    "    \"\"\"One level of Haar DWT. Input length must be even.\"\"\"\n",
    "    n = len(x) - len(x) % 2   # truncate to even\n",
    "    x = x[:n]\n",
    "    approx = (x[0::2] + x[1::2]) / np.sqrt(2)\n",
    "    detail = (x[0::2] - x[1::2]) / np.sqrt(2)\n",
    "    return approx, detail\n",
    "\n",
    "\n",
    "def haar_dwt_multi(x, levels=5):\n",
    "    \"\"\"Multi-level Haar DWT. Returns list of detail coeffs + final approx.\"\"\"\n",
    "    details = []\n",
    "    current = x.copy()\n",
    "    for _ in range(levels):\n",
    "        current, detail = haar_dwt_1level(current)\n",
    "        details.append(detail)\n",
    "    return current, details   # approx, [D1, D2, ..., D_levels]\n",
    "\n",
    "\n",
    "# Compute DWT for Sensor_00\n",
    "LEVELS = 6\n",
    "approx, details = haar_dwt_multi(df['Sensor_00'].values, levels=LEVELS)\n",
    "\n",
    "# Energy in sliding windows per level\n",
    "WIN = 128\n",
    "energy_map = []\n",
    "for lvl, d in enumerate(details):\n",
    "    energies = []\n",
    "    for i in range(0, len(d) - WIN, WIN // 2):\n",
    "        energies.append(np.sum(d[i:i+WIN]**2))\n",
    "    energy_map.append(energies)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "img = ax.imshow(energy_map, aspect='auto', cmap='inferno', origin='lower')\n",
    "ax.set_ylabel('Decomposition Level (1=high freq)')\n",
    "ax.set_xlabel('Time Window Index')\n",
    "ax.set_title('Wavelet Energy Map — Sensor_00 (Haar DWT)', fontsize=13)\n",
    "plt.colorbar(img, ax=ax, label='Energy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb7425",
   "metadata": {},
   "source": [
    "<!-- 3. Transfer Entropy — Causal Information Flow Between Sensors\n",
    "\n",
    "Transfer entropy (TE) quantifies **directed** information flow. how much knowing the past of sensor A reduces uncertainty about the future of sensor B, beyond B's own past. This goes beyond correlation to detect **asymmetric causality**. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6. Discretised Transfer Entropy (binned estimator) ──\n",
    "def transfer_entropy(x, y, lag=3, n_bins=8):\n",
    "    \"\"\"\n",
    "    Estimate TE(X → Y): how much X's past helps predict Y's future\n",
    "    beyond Y's own past.  Binned (histogram-based) estimator.\n",
    "    \"\"\"\n",
    "    n = len(x) - lag\n",
    "    # Build joint vectors\n",
    "    x_past = x[:n]\n",
    "    y_past = y[:n]\n",
    "    y_fut = y[lag:lag+n]\n",
    "\n",
    "    # Discretise\n",
    "    def digitise(arr):\n",
    "        return np.digitize(arr, np.histogram_bin_edges(arr, bins=n_bins)[1:-1])\n",
    "\n",
    "    xp = digitise(x_past)\n",
    "    yp = digitise(y_past)\n",
    "    yf = digitise(y_fut)\n",
    "\n",
    "    # Counts → probabilities (with Laplace smoothing)\n",
    "    from collections import Counter\n",
    "\n",
    "    def prob(keys):\n",
    "        counts = Counter(keys)\n",
    "        total = sum(counts.values()) + len(set(keys))  # smoothing\n",
    "        return {k: (v+1)/total for k, v in counts.items()}\n",
    "\n",
    "    # Joint and marginal probabilities\n",
    "    p_yp_yf = prob(list(zip(yp, yf)))\n",
    "    p_xp_yp_yf = prob(list(zip(xp, yp, yf)))\n",
    "    p_yp = prob(list(yp))\n",
    "    p_xp_yp = prob(list(zip(xp, yp)))\n",
    "\n",
    "    te = 0.0\n",
    "    for i in range(n):\n",
    "        key3 = (xp[i], yp[i], yf[i])\n",
    "        key2a = (yp[i], yf[i])\n",
    "        key2b = (xp[i], yp[i])\n",
    "        key1 = yp[i]\n",
    "        p3 = p_xp_yp_yf.get(key3, 1e-10)\n",
    "        p2a = p_yp_yf.get(key2a, 1e-10)\n",
    "        p2b = p_xp_yp.get(key2b, 1e-10)\n",
    "        p1 = p_yp.get(key1, 1e-10)\n",
    "        te += p3 * np.log2(p3 * p1 / (p2a * p2b))\n",
    "    return te / n\n",
    "\n",
    "\n",
    "# Compute TE for the 4-sensor cluster (0-3) — subsample for speed\n",
    "SUB = 3000\n",
    "rng = np.random.default_rng(0)\n",
    "idx = np.sort(rng.choice(len(df), SUB, replace=False))\n",
    "sub = df[sensor_cols[:4]].iloc[idx].values\n",
    "\n",
    "print('Computing Transfer Entropy (4 sensors, subsample) …')\n",
    "n_s = 4\n",
    "te_matrix = np.zeros((n_s, n_s))\n",
    "for i in range(n_s):\n",
    "    for j in range(n_s):\n",
    "        if i != j:\n",
    "            te_matrix[i, j] = transfer_entropy(\n",
    "                sub[:, i], sub[:, j], lag=5, n_bins=6)\n",
    "            print(\n",
    "                f'  TE({sensor_cols[i]} → {sensor_cols[j]}) = {te_matrix[i, j]:.4f}')\n",
    "\n",
    "te_df = pd.DataFrame(te_matrix, index=sensor_cols[:4], columns=sensor_cols[:4])\n",
    "print('\\nTransfer Entropy Matrix:')\n",
    "te_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7. Visualise TE as a directed heatmap ───────────────\n",
    "fig, ax = plt.subplots(figsize=(7, 5.5))\n",
    "sns.heatmap(te_df, annot=True, fmt='.4f', cmap='YlGnBu', ax=ax,\n",
    "            linewidths=0.5, cbar_kws={'label': 'Transfer Entropy (bits)'})\n",
    "ax.set_title('Transfer Entropy (row → col)', fontsize=13)\n",
    "ax.set_xlabel('Target Sensor')\n",
    "ax.set_ylabel('Source Sensor')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 4. Granger Causality — VAR-based Pairwise Test\n",
    "\n",
    "Granger causality tests whether lagged values of sensor X provide **statistically significant** improvement in predicting sensor Y using an F-test on nested VAR models. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8. Granger Causality (manual VAR F-test) ────────────\n",
    "from scipy.stats import f as f_dist\n",
    "from numpy.linalg import lstsq\n",
    "\n",
    "\n",
    "def granger_f_test(y, x, max_lag=8):\n",
    "    \"\"\"\n",
    "    F-test: does x Granger-cause y?\n",
    "    Compares:\n",
    "      H0 (restricted):  y_t = a1*y_{t-1} + ... + a_p*y_{t-p}\n",
    "      H1 (full):         + b1*x_{t-1} + ... + b_p*x_{t-p}\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    p = max_lag\n",
    "    # Build matrices\n",
    "    Y_dep = y[p:]\n",
    "    # Restricted (y lags only)\n",
    "    X_r = np.column_stack([y[p-i-1:n-i-1] for i in range(p)])\n",
    "    # Full (y + x lags)\n",
    "    X_f = np.column_stack([X_r,\n",
    "                           *[x[p-i-1:n-i-1] for i in range(p)]])\n",
    "\n",
    "    # OLS\n",
    "    beta_r, res_r, _, _ = lstsq(np.column_stack(\n",
    "        [X_r, np.ones(len(Y_dep))]), Y_dep, rcond=None)\n",
    "    beta_f, res_f, _, _ = lstsq(np.column_stack(\n",
    "        [X_f, np.ones(len(Y_dep))]), Y_dep, rcond=None)\n",
    "\n",
    "    # RSS\n",
    "    rss_r = np.sum(\n",
    "        (Y_dep - np.column_stack([X_r, np.ones(len(Y_dep))]) @ beta_r)**2)\n",
    "    rss_f = np.sum(\n",
    "        (Y_dep - np.column_stack([X_f, np.ones(len(Y_dep))]) @ beta_f)**2)\n",
    "\n",
    "    # F statistic\n",
    "    q = p                          # number of restrictions\n",
    "    k = X_f.shape[1] + 1           # full model params\n",
    "    T = len(Y_dep)\n",
    "    F = ((rss_r - rss_f) / q) / (rss_f / (T - k))\n",
    "    return F\n",
    "\n",
    "\n",
    "# Use first 5000 rows for speed\n",
    "sub5 = df[sensor_cols[:6]].values[:5000]\n",
    "n_sens = 6\n",
    "gran_F = np.full((n_sens, n_sens), np.nan)\n",
    "gran_p = np.full((n_sens, n_sens), np.nan)\n",
    "LAG = 8\n",
    "\n",
    "print('Running Granger F-tests (6 sensors) …')\n",
    "for i in range(n_sens):\n",
    "    for j in range(n_sens):\n",
    "        if i != j:\n",
    "            F_val = granger_f_test(sub5[:, j], sub5[:, i], max_lag=LAG)\n",
    "            gran_F[i, j] = F_val\n",
    "            # p-value: F(LAG, T - 2*LAG - 1)\n",
    "            df_num = LAG\n",
    "            df_den = len(sub5) - 2*LAG - 1\n",
    "            gran_p[i, j] = 1 - f_dist.cdf(F_val, df_num, df_den)\n",
    "\n",
    "gran_F_df = pd.DataFrame(\n",
    "    gran_F, index=sensor_cols[:6], columns=sensor_cols[:6])\n",
    "gran_p_df = pd.DataFrame(\n",
    "    gran_p, index=sensor_cols[:6], columns=sensor_cols[:6])\n",
    "print('\\nGranger F-statistic matrix (row → col):')\n",
    "gran_F_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9. Granger significance heatmap (α = 0.05) ─────────\n",
    "sig = (gran_p_df < 0.05).astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F-statistic\n",
    "sns.heatmap(gran_F_df.round(1), annot=True, fmt='.1f', cmap='Reds',\n",
    "            ax=axes[0], linewidths=0.5, cbar_kws={'label': 'F-statistic'})\n",
    "axes[0].set_title('Granger F-statistic (row → col)')\n",
    "axes[0].set_xlabel('Target')\n",
    "axes[0].set_ylabel('Source')\n",
    "\n",
    "# Significance map\n",
    "sns.heatmap(sig, annot=True, fmt='d', cmap='RdYlGn', vmin=0, vmax=1,\n",
    "            ax=axes[1], linewidths=0.5, cbar_kws={'label': 'Significant (α<0.05)'})\n",
    "axes[1].set_title('Significant Causal Links')\n",
    "axes[1].set_xlabel('Target')\n",
    "axes[1].set_ylabel('Source')\n",
    "\n",
    "plt.suptitle('Granger Causality Analysis', fontsize=14, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 5. EWMA Control Chart — Adaptive Threshold\n",
    "\n",
    "Unlike fixed 3σ limits, EWMA **exponentially weights recent observations**, making it highly sensitive to small persistent shifts. We overlay it on a sensor with an injected gradual drift. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10. Inject a subtle drift and apply EWMA control ────\n",
    "rng = np.random.default_rng(77)\n",
    "n = len(df)\n",
    "signal = df['Sensor_00'].values.copy()\n",
    "\n",
    "# Inject subtle +0.8 drift starting at index 12000\n",
    "drift_start = 12000\n",
    "signal[drift_start:] += np.linspace(0, 0.8, n - drift_start)\n",
    "\n",
    "# EWMA parameters\n",
    "lam = 0.1      # smoothing factor (lower = more memory)\n",
    "mu = np.mean(signal[:drift_start])\n",
    "sigma = np.std(signal[:drift_start])\n",
    "L = 2.7      # control-limit width factor\n",
    "\n",
    "ewma = np.zeros(n)\n",
    "ewma[0] = signal[0]\n",
    "ucl = np.zeros(n)\n",
    "lcl = np.zeros(n)\n",
    "\n",
    "for i in range(1, n):\n",
    "    ewma[i] = lam * signal[i] + (1 - lam) * ewma[i-1]\n",
    "    # EWMA variance grows then saturates\n",
    "    var_ewma = (sigma**2 * lam / (2 - lam)) * (1 - (1 - lam)**(2*(i+1)))\n",
    "    ucl[i] = mu + L * np.sqrt(var_ewma)\n",
    "    lcl[i] = mu - L * np.sqrt(var_ewma)\n",
    "\n",
    "alerts = (ewma > ucl) | (ewma < lcl)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 7), sharex=True)\n",
    "\n",
    "# Raw + EWMA\n",
    "axes[0].plot(signal, lw=0.5, color='steelblue', alpha=0.5, label='Raw')\n",
    "axes[0].plot(ewma,   lw=1.2, color='darkblue',            label='EWMA')\n",
    "axes[0].axvline(drift_start, color='red', ls=':',\n",
    "                lw=1.5, label='Drift injected')\n",
    "axes[0].set_title('EWMA-Smoothed Signal vs Raw', fontsize=12)\n",
    "axes[0].set_ylabel('Response')\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# EWMA with control limits\n",
    "axes[1].plot(ewma, lw=1.2, color='darkblue', label='EWMA')\n",
    "axes[1].fill_between(range(n), lcl, ucl, color='green',\n",
    "                     alpha=0.08, label='Control band')\n",
    "axes[1].plot(ucl, color='red',   ls='--', lw=0.8)\n",
    "axes[1].plot(lcl, color='red',   ls='--', lw=0.8)\n",
    "axes[1].scatter(np.where(alerts)[0], ewma[alerts], s=12,\n",
    "                color='red', zorder=5, label=f'Alerts ({alerts.sum()})')\n",
    "axes[1].axvline(drift_start, color='red', ls=':', lw=1.5)\n",
    "axes[1].set_title('EWMA Control Chart (adaptive limits)', fontsize=12)\n",
    "axes[1].set_ylabel('EWMA')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\n",
    "    f'First alert at index {np.argmax(alerts)} vs drift at {drift_start} → detection lag = {np.argmax(alerts)-drift_start} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8834adc",
   "metadata": {},
   "source": [
    "<!-- 6. Sensor Fusion — PCA + t-SNE Joint State Embedding\n",
    "\n",
    "We define operating states by sliding-window statistics (mean, std, skewness, kurtosis) across all 16 sensors, then project into 2-D using t-SNE to visualise cluster structure. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 11. Sliding-window feature extraction ──────────────\n",
    "WIN = 200\n",
    "STEP = 100\n",
    "rows = []\n",
    "for start in range(0, len(df) - WIN, STEP):\n",
    "    chunk = df[sensor_cols].iloc[start:start+WIN]\n",
    "    feat = {}\n",
    "    feat['window_start'] = start\n",
    "    for col in sensor_cols:\n",
    "        s = chunk[col]\n",
    "        feat[f'{col}_mean'] = s.mean()\n",
    "        feat[f'{col}_std'] = s.std()\n",
    "        feat[f'{col}_skew'] = s.skew()\n",
    "        feat[f'{col}_kurt'] = s.kurtosis()\n",
    "    rows.append(feat)\n",
    "\n",
    "feat_df = pd.DataFrame(rows)\n",
    "print(f'Feature matrix: {feat_df.shape}')\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 12. PCA → t-SNE two-stage embedding ─────────────────\n",
    "feat_cols = [c for c in feat_df.columns if c != 'window_start']\n",
    "scaler = StandardScaler()\n",
    "X_s = scaler.fit_transform(feat_df[feat_cols])\n",
    "\n",
    "# PCA to 30 dims first (speeds up t-SNE dramatically)\n",
    "pca30 = PCA(n_components=30, random_state=0)\n",
    "X_pca = pca30.fit_transform(X_s)\n",
    "print(\n",
    "    f'PCA 30-d explained variance: {pca30.explained_variance_ratio_.sum()*100:.1f}%')\n",
    "\n",
    "# t-SNE to 2-D\n",
    "tsne = TSNE(n_components=2, perplexity=40, learning_rate=200,\n",
    "            n_iter=1000, random_state=0, metric='euclidean')\n",
    "X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "# Colour by the dominant sensor's rolling std (proxy for activity level)\n",
    "activity = feat_df[[f'{s}_std' for s in sensor_cols[:4]]].mean(axis=1).values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sc = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=activity, cmap='plasma',\n",
    "                s=15, alpha=0.7, edgecolors='none')\n",
    "plt.colorbar(sc, ax=ax, label='Mean Activity (Std of top-4 sensors)')\n",
    "ax.set_title('t-SNE Embedding of 16-Sensor Window Features', fontsize=13)\n",
    "ax.set_xlabel('t-SNE dim 1')\n",
    "ax.set_ylabel('t-SNE dim 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 13. K-Means clustering on the embedding ─────────────\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = 5\n",
    "km = KMeans(n_clusters=K, random_state=0, n_init=10)\n",
    "labels = km.fit_predict(X_tsne)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# t-SNE coloured by cluster\n",
    "scatter = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels,\n",
    "                          cmap='tab10', s=15, alpha=0.7, edgecolors='none')\n",
    "axes[0].set_title(f't-SNE with K-Means (K={K})')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Cluster sizes + mean activity\n",
    "cluster_activity = pd.DataFrame({'Cluster': labels, 'Activity': activity})\n",
    "ca_mean = cluster_activity.groupby(\n",
    "    'Cluster')['Activity'].agg(['mean', 'count'])\n",
    "ca_mean['mean'].plot(kind='bar', ax=axes[1],\n",
    "                     color='steelblue', edgecolor='white')\n",
    "axes[1].set_title('Mean Activity per Cluster')\n",
    "axes[1].set_ylabel('Activity Level')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "for i, (m, c) in enumerate(ca_mean.itertuples(index=False)):\n",
    "    axes[1].text(i, m+0.002, f'n={int(c)}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Portfolio Takeaways\n",
    "\n",
    "| Technique | Insight Gained |\n",
    "|---|---|\n",
    "| **Welch PSD** | Each sensor has 1-3 dominant frequencies; cluster members share spectral signatures |\n",
    "| **Haar DWT** | Transient gas-pulse events appear as high-energy bursts at detail levels 2-4 |\n",
    "| **Transfer Entropy** | Asymmetric causal links within sensor clusters; identifies driver sensors |\n",
    "| **Granger Causality** | Statistically significant directed relationships validated with F-tests |\n",
    "| **EWMA Control** | Detects subtle drifts within ~20 samples — far earlier than 3σ charts |\n",
    "| **PCA + t-SNE + KMeans** | Sensor network has 4-5 distinct operating regimes; activity-level clustering |\n",
    "\n",
    "These techniques form the analytical backbone for **predictive maintenance**, **sensor-network health monitoring**, and **root-cause isolation** in industrial IoT systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
